{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.models import resnet50\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchvision.transforms.functional as F\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(torch.cuda.is_available())\n",
    "# print(torch.cuda.current_device())\n",
    "# print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "# print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "\n",
    "device = torch.device(torch.cuda.current_device() if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset_dir = \"dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get image size from the first image\n",
    "first_image_file = os.listdir(f\"{dataset_dir}/Non Demented\")[0]\n",
    "img = plt.imread(f\"{dataset_dir}/Non Demented/{first_image_file}\")\n",
    "\n",
    "img_height, img_width, _ = img.shape\n",
    "\n",
    "print(f\"Image size: {img_height}x{img_width}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch size\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "data_transforms = transforms.Compose([\n",
    "    # transforms.Resize((250, 250)),\n",
    "    transforms.RandomResizedCrop(img_height),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the datasets\n",
    "dataset = ImageFolder(dataset_dir, transform=data_transforms)\n",
    "\n",
    "# Create train and test data splits\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classes\n",
    "num_classes = len(dataset.classes)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of epochs\n",
    "epochs_number = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing pretrained ResNet-50 model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ResNet50 model\n",
    "base_resNet_model = resnet50()\n",
    "\n",
    "# Freeze convolutional layers\n",
    "for param in base_resNet_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Add custom classification head\n",
    "num_ftrs = base_resNet_model.fc.in_features\n",
    "base_resNet_model.fc = nn.Sequential(\n",
    "    nn.Linear(num_ftrs, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(512, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(256, num_classes),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(base_resNet_model.fc.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store accuracy after each epoch\n",
    "train_accuracy = []\n",
    "val_accuracy = []\n",
    "\n",
    "# Move model to device\n",
    "base_resNet_model = base_resNet_model.to(device)\n",
    "\n",
    "# Train model\n",
    "for epoch in tqdm(range(epochs_number), desc=\"Epochs\"):\n",
    "    # start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Training loop\n",
    "    for i, (inputs, labels) in enumerate(train_loader, start=1):\n",
    "        # Move input and label tensors to the default device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = base_resNet_model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Print file number\n",
    "        print(f\"\\r[{i}/{len(train_loader)}]\", end=\"\")\n",
    "\n",
    "    # Compute accuracy\n",
    "    train_acc = correct / total\n",
    "    train_accuracy.append(train_acc)\n",
    "\n",
    "    # Validation loop\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = base_resNet_model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Compute accuracy\n",
    "    val_acc = correct / total\n",
    "    val_accuracy.append(val_acc)\n",
    "\n",
    "    # Print statistics\n",
    "    print(f\"Train loss: {running_loss/len(train_loader):.3f} | Train accuracy: {train_acc:.3f} | Validation accuracy: {val_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "torch.save(base_resNet_model.state_dict(), \"resnet.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation accuracy\n",
    "plt.plot(train_accuracy, label=\"Training accuracy\")\n",
    "plt.plot(val_accuracy, label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy - ResNet50V2\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load model\n",
    "# base_resNet_model.load_state_dict(torch.load(\"resnet.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing DenseNet-121 model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load pre-trained DenseNet121 model\n",
    "# base_denseNet_model = DenseNet121(weights=\"imagenet\", include_top=False, input_shape=(img_height, img_width, 3))\n",
    "\n",
    "# # Add custom classification head\n",
    "# x = base_denseNet_model.output\n",
    "# x = GlobalAveragePooling2D()(x)\n",
    "# x = Dense(1024)(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Activation(\"relu\")(x)\n",
    "# x = Dropout(0.3)(x)\n",
    "# predictions = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "# # Create model\n",
    "# denseNet_model = Model(inputs=base_denseNet_model.input, outputs=predictions)\n",
    "\n",
    "# # Freeze convolutional layers\n",
    "# for layer in base_denseNet_model.layers:\n",
    "#     layer.trainable = False\n",
    "\n",
    "# # Compile model\n",
    "# optimizer = Adam(learning_rate=0.001)\n",
    "# denseNet_model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "# # denseNet_model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\", f1_m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train model\n",
    "# denseNet_history = denseNet_model.fit(train_generator, steps_per_epoch=train_generator.samples // batch_size, epochs=epochs_number, validation_data=test_generator, validation_steps=test_generator.samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save trained model\n",
    "# denseNet_model.save(\"denseNet_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(denseNet_history.history[\"accuracy\"], label=\"Training accuracy\")\n",
    "# plt.plot(denseNet_history.history[\"val_accuracy\"], label=\"Validation accuracy\")\n",
    "# plt.title(\"Training and validation accuracy - DenseNet121\")\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing EfficientNetB7 model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the ResNet50 model\n",
    "# base_efficientNet_model = EfficientNetB7(weights=\"imagenet\", include_top=False, input_shape=(img_height, img_width, 3))\n",
    "\n",
    "# # Add custom classification head\n",
    "# x = base_efficientNet_model.output\n",
    "# x = GlobalAveragePooling2D()(x)\n",
    "# x = Dense(1024)(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Activation(\"relu\")(x)\n",
    "# x = Dropout(0.3)(x)\n",
    "# predictions = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "# # Create model\n",
    "# efficientNet_model = Model(inputs=base_efficientNet_model.input, outputs=predictions)\n",
    "\n",
    "# # Freeze convolutional layers\n",
    "# for layer in base_efficientNet_model.layers:\n",
    "#     layer.trainable = False\n",
    "\n",
    "# # Compile model\n",
    "# optimizer = Adam(learning_rate=0.001)\n",
    "# efficientNet_model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "# # efficientNet_model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\", f1_m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the model\n",
    "# efficientNet_history = efficientNet_model.fit(train_generator, steps_per_epoch=train_generator.samples // batch_size, epochs=epochs_number, validation_data=test_generator, validation_steps=test_generator.samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the trained model\n",
    "# efficientNet_model.save(\"efficientNet_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing custom CNN model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocess data\n",
    "# data_transforms = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomRotation(10),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "# ])\n",
    "\n",
    "# dataset = ImageFolder(dataset_dir, transform=data_transforms)\n",
    "\n",
    "# # Define the train-test split\n",
    "# train_size = int(0.8 * len(dataset))\n",
    "# test_size = len(dataset) - train_size\n",
    "\n",
    "# train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# val_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_iter = iter(train_loader)\n",
    "# images, labels = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean = np.array([0.485, 0.456, 0.406])\n",
    "# std = np.array([0.229, 0.224, 0.225])\n",
    "# images = (images.numpy().transpose((0, 2, 3, 1)) * std + mean).clip(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TumorClassifier(nn.Module):\n",
    "#     def __init__(self, num_classes):\n",
    "#         super(TumorClassifier, self).__init__()\n",
    "#         self.features = nn.Sequential(\n",
    "#             nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#             nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         )\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(32 * 56 * 56, 128),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Linear(128, num_classes)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.features(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.classifier(x)\n",
    "#         return x\n",
    "\n",
    "# model = TumorClassifier(num_classes=num_classes)\n",
    "# # Load a pre-trained ResNet model and modify the classifier\n",
    "# model.to(device)\n",
    "\n",
    "# # Define loss function and optimizer\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize lists to store training history\n",
    "# train_losses = []\n",
    "# val_losses = []\n",
    "# train_accuracies = []\n",
    "# val_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training loop\n",
    "# num_epochs = 20\n",
    "# best_val_accuracy = 0.0\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     train_loss = 0.0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "    \n",
    "#     for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "#         inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         train_loss += loss.item()\n",
    "#         _, predicted = torch.max(outputs, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "    \n",
    "#     train_accuracy = correct / total\n",
    "#     train_losses.append(train_loss)\n",
    "#     train_accuracies.append(train_accuracy)\n",
    "    \n",
    "#     # Validation\n",
    "#     model.eval()\n",
    "#     val_loss = 0.0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for inputs, labels in val_loader:\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "            \n",
    "#             val_loss += loss.item()\n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "            \n",
    "#     val_loss /= len(val_loader)\n",
    "#     val_accuracy = correct / total\n",
    "#     val_losses.append(val_loss)\n",
    "#     val_accuracies.append(val_accuracy)\n",
    "    \n",
    "#     print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "#           f'Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2%}, '\n",
    "#           f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2%}')\n",
    "\n",
    "    \n",
    "#     # Save the best model\n",
    "#     if val_accuracy > best_val_accuracy:\n",
    "#         best_val_accuracy = val_accuracy\n",
    "#         torch.save(model.state_dict(), 'best_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorflow Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Custom CNN architecture\n",
    "# custom_model = Sequential(\n",
    "#     [\n",
    "#         Conv2D(32, (3, 3), activation=\"relu\", input_shape=(img_height, img_width, 3)),\n",
    "#         MaxPooling2D((2, 2)),\n",
    "#         Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "#         MaxPooling2D((2, 2)),\n",
    "#         Conv2D(128, (3, 3), activation=\"relu\"),\n",
    "#         MaxPooling2D((2, 2)),\n",
    "#         Conv2D(128, (3, 3), activation=\"relu\"),\n",
    "#         MaxPooling2D((2, 2)),\n",
    "#         Flatten(),\n",
    "#         Dense(512, activation=\"relu\"),\n",
    "#         Dropout(0.5),\n",
    "#         Dense(num_classes, activation=\"softmax\"),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # Compile the model\n",
    "# optimizer = Adam(learning_rate=0.001)\n",
    "# custom_model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "# # custom_model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\", f1_m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the model\n",
    "# custom_CNN_history = custom_model.fit(train_generator, steps_per_epoch=train_generator.samples // batch_size, epochs=epochs_number, validation_data=test_generator, validation_steps=test_generator.samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the trained model\n",
    "# custom_model.save(\"custom_CNN_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(custom_CNN_history.history[\"accuracy\"], label=\"Training accuracy\")\n",
    "# plt.plot(custom_CNN_history.history[\"val_accuracy\"], label=\"Validation accuracy\")\n",
    "# plt.title(\"Training and validation accuracy - Custom CNN\")\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Table comparing the performance of models\n",
    "# resNet_accuracy = resNet_history.history[\"accuracy\"][-1]\n",
    "# resNet_val_accuracy = resNet_history.history[\"val_accuracy\"][-1]\n",
    "# denseNet_accuracy = denseNet_history.history[\"accuracy\"][-1]\n",
    "# denseNet_val_accuracy = denseNet_history.history[\"val_accuracy\"][-1]\n",
    "# custom_CNN_accuracy = custom_CNN_history.history[\"accuracy\"][-1]\n",
    "# custom_CNN_val_accuracy = custom_CNN_history.history[\"val_accuracy\"][-1]\n",
    "# efficientNet_accuracy = efficientNet_history.history[\"accuracy\"][-1]\n",
    "# efficientNet_val_accuracy = efficientNet_history.history[\"val_accuracy\"][-1]\n",
    "\n",
    "# # resNet_f1 = resNet_history.history[\"f1_m\"][-1]\n",
    "# # resNet_val_f1 = resNet_history.history[\"val_f1_m\"][-1]\n",
    "# # denseNet_f1 = denseNet_history.history[\"f1_m\"][-1]\n",
    "# # denseNet_val_f1 = denseNet_history.history[\"val_f1_m\"][-1]\n",
    "# # custom_CNN_f1 = custom_CNN_history.history[\"f1_m\"][-1]\n",
    "# # custom_CNN_val_f1 = custom_CNN_history.history[\"val_f1_m\"][-1]\n",
    "# # efficientNet_f1 = efficientNet_history.history[\"f1_m\"][-1]\n",
    "# # efficientNet_val_f1 = efficientNet_history.history[\"val_f1_m\"][-1]\n",
    "\n",
    "# model_comparison = pd.DataFrame(\n",
    "#     {\n",
    "#         \"Model\": [\"ResNet50V2\", \"DenseNet121\", \"Custom CNN\", \"EfficientNetB7\"],\n",
    "#         \"Train Accuracy\": [resNet_accuracy, denseNet_accuracy, custom_CNN_accuracy, efficientNet_accuracy],\n",
    "#         \"Validation Accuracy\": [resNet_val_accuracy, denseNet_val_accuracy, custom_CNN_val_accuracy, efficientNet_val_accuracy],\n",
    "#         # \"Training F1 Score\": [resNet_f1, denseNet_f1, custom_CNN_f1, efficientNet_f1],\n",
    "#         # \"Validation F1 Score\": [resNet_val_f1, denseNet_val_f1, custom_CNN_val_f1, efficientNet_val_f1],\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# model_comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
